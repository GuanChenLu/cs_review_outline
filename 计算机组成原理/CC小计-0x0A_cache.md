# Cache 存储器

## Cache 的基本原理

### cache 的功能

`cache` 被夹在 `CPU` 与 `主存` 之间，用于解决两者速度不匹配，基于 `空间局部性 和 时间局部性` 原理

能够使用多次 `cache` 进一步提高

![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220406200336.png)





### cache 的基本原理

![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220406200442.png)

1. `CPU` 和 cache 之间是按照字进行数据交换的，cache 与 主存之间则是以由若干个 字 组成的 块经行数据交换的
2. 当 CPU 需要读取一个字的时候，将地址发送到 cache 和主存
3. cache 控制逻辑根据地址判断此字是否在当前的 cache 中，如果是，就从 cache 中直接返回；如果不存在，用主存中的读周期将此字从内存中读出送到 `CPU`，与此同时将含有这个字的整个数据块从内存中读到 cache 中



###  cache 的命中率

在一个程序执行期间，设 Nc 表示 cache 完成存取的总次数，Nm 表示主存完成存取的总次数，h 定义为命中率，则有

![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220406201228.png)

![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220406201315.png)

我们追求的是让总时间 `ta` 越接近 `tc` 越好

![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220406201447.png)

为了提高访问效率，命中率 `h` 越接近 `1` 越好，`r` 以 `5 ~ 10` 为宜

`h` 与程序的行为，`cache` 的容量，组织方式，块大小有关



### cache 设计需要解决的问题

应该遵循的原则：

1. 命中率尽量高
2. **cache 对于 CPU 而言是透明的，访问 cache 还是 主存 的方法一样**

为了实现上面的 target，需要增加 硬件电路 完成控制功能，即 cache 控制器

需要解决以下几个问题

1. 主存内容调入 cache 时如何存放
2. 发出访存地址时如何映射到 cache 中
3. cache 空间不足时如何替换掉里面的内容
4. 写操作时如何改写 cache 中的内容

前两个问题是相关的，实际上就是如何将主存信息定位在 cache 中，怎么样把主存地址变换为 cache 地址，

cache 容量与主存相比较小，且他们之间数据交换是以  块 为单位的

为了将主存地址定位到 `cache` 中，我们使用 **地址映射**，将 CPU 访问存储器时的 **一个字的存储地址** 自动变换为 cache 的地址，实现 cache 地址变换

cache 替换问题主要是选择和执行替换算法，以便在 cache 不命中时替换 cache 

最后一个问题涉及到 cache 的写操作策略，重点是在更新时保证主存和 cache 的一致性





## 主存与 cache 的映射方式

主要有三种，具体体现在 比较器中 的实现逻辑 不同

1. 全相联映射模式
2. 直接映射模式
3. 组相联模式

### 全相联映射

前面我们说过，主存中和 cache 中的数据是按照 块 来交换的

在 `cache` 中，块的大小被称作为 `行`

一个 cache 包含许多个 行



在 主存中，数据块的大小就称为 块，与 cache 中的行可以一一对应



在全相联映射中，将主存中一个 块(块号) 的地址与块中的内容(几个字) 一起存放在 cache 的行中，其中块地址存于 cache 行的标记(tag)中



![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220406205615.png)



![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220406205802.png)

可以直接通过前 S 位定位到块，如果块号命中，则按照后面给出的 W 个字地址从 cache 中取出一个字

如果块号未命中，则从主存中取 **这个块** 到 cache 的对应的行上

优先很显然(cv)：

可使主存的一个块直接拷贝到cache中的任意一行上，非常灵活，cache空间的利用率高，cache的块冲突概率低  



可以看到，为了实现所有块的比较，比较器需要集成所有的块标记，对于全部的数据的存储则是通过普通的 RAM 来实现的

这样结构的主要缺点是高速比较器比较难于设计和实现，因此只适用于小容量的 cache 实现



### 直接映射方式

在 直接映射方式 中，`cache -> 主存` 是一种 `一对多` 的关系

就像是 hash 表的实现方式一样，cache 中行号 i 和主存中的块号 j 有下面的关系：

`i = j mod m (m 是 cache 中的总行数)`

我们不妨将 `一片在主存中可以完整得映射到 cache 中所有行的块的集合` 称为 `一个区`

![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220406211153.png)



我们在 `cache` 中还是要有 `S` 位来记录主存块地址，只不过在这里将 `S` 位主存块地址分为两个部分

- 低 r 位表示的是在 cache 中的第几行，在主存中体现的就是某个区的第几块
- 高 s - r 位是在主存中映射的区号，与块数据一起保存

验证一下：`2^r（每个区中的块数量） * 2^(s - r)(总共分成 s - r 块) = 2^s = 总块数`

![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220406212508.png)



首先通过 r 位找到在 cache 中的特定的一行

然后使用 s - r 位区号部分和此行的标记在比较器中比较



也即是说，cache 中的一行对应的是主存中特定的某一区中的某一块，我们首先要取得 cache 中的对应的行数，通过 r 位

然后，由于 cache 中的哪一行 s 位标记中的高 s - r 位标记此行中的数据是从主存中的哪一个 区 中读取出来的，所以我们要将这 s - r 位继续和传入地址的 s - r 位作比较，如果相同就能说明目标区和 cache 这一行数据在主存中的区号相同，即 cache 命中

可以看出，由于只要比较 s - r 位表示的区号，这种方式设计的比较器会是比较简单的



优点：

1. 硬件简单，成本低，地址变换的速度快



缺点：

1. **每个主存块只有在映射到 cache 中之后，只有一个固定的位置(行)可以存放**，发生 hash 冲突之后，需要将原先存入的行换出去，但是有可能经过一段时间后又要换入，平凡地换入换出会导致 cache 的效率下降



因此，行数更多的 cache 可以减小冲突的机会



### 组相联映射方式

以上两种映射方式各有千秋，从存放位置和灵活性和命中率来说，全相联模式好，从比较器电路的简单以及硬件的投资来说，直接映射为佳

在分析直接映射的缺点的时候，我没说过，hash过一样的不同区中的块都会被映射到 cache 中的**同一且唯一行**中

在组相联映射方式中，我们 **扩充 cache 中所有行 用来存储主存中块的范围，使 cache 中的每一行够可以存储主存块中的 v 个块**，我们将改造成的能够存储多个块的“那一行”称为 **组**

这样，原来通过 hash 映射后找到的 cache 中的行号就变成了现在的 **组号**



**主存中的某一块都可以放入 cache 特定一组的任意行中**

存放在哪个组中是通过 hash 出来的，至于存放到哪一行中是灵活的



![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220406221416.png)



**组间采用直接映射，组内采用全相联**，这样的话，在输入低 r 位确定是哪个组之后，将改组中所有“行”的 `s - r` 位区号输入到比较器中和给出地址进行比较

![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220406221439.png)





## Cache 的替换策略

由于 cache 需要保存最新的数据，而受限制与空间大小，所以必须要产生替换

对于直接映射法经行地址映射的 cache ，不涉及替换算法

对于全映射和组相联进行映射的算法，在同一组(行)中会存在从主存中映射到的不同数据，需要考虑数据的替换

下面介绍三种最常用的算法

### LFU

采用最近最不经常使用的策略

对于 cache 中的每一行，设置一个计数器，每访问一次，将计数器的数值 + 1，当需要替换的时候，将计数最小的行替换出去

采用这种算法将周期限定在两次替换之间的时间间隔内。如果有新刷入的数据还没来得及被使用就会被淘汰掉



### LRU

这个算法相信很多人都是很熟悉的，就是将 cache 中长久没有访问的数据换出

与 `LFU` 的原理(实现)恰好相反，这种算法将命中的行的计数器 清零，其他行的计数器 + 1，将计数值最大的行换出

在 `mysql` 的 `buffer pool`缓存中的也采用了这种算法，只不过是加上了分代机制和换代时间来优化，关于 `buffer pool` 的解释我暂时挂在我的 

(个人网站首页上)[http://124.222.2.79/#/]

这种做法保护了 cache 中的新数据，且有高效的命中率，使用广泛，但是硬件实现较为复杂

我们以一个例子来说明这种算法

![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220411220010.png)

还记得我们前面说过的，采用组相联的映射方式，主存地址通过和组数取余的方式找到将要存放入的组中(直接映射)，然后在组中通过全相联的模式匹配

在找到组之后，如果组中存放的数据已满，就会触发替换机制，`LRU` 就是在这个时候发挥作用的

![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220411220500.png)





### 随机替换

从特定的行位置中随机选一组换出 

这种方式虽然听上去比较猴，但是实现起来比较简单，效果也不错



## Cache 中的写操作

`cache` 是主存内容的拷贝，应当与主存的内容保持一致

下面介绍三种逐步演进的写操作策略

### 写回 (wriite back, copy back)

这里写回的对象是指 cache

就是说，CPU 写 cache 命中时，**只修改** cache 中的内容，而不是立即写入主存，只有当该 cache 被换出时才会写入主存

既然写回的对象不是主存，那么 **当写 cache 未命中的时候(等于说该地址的内存片段没有加载到 cache 中)，需要从 主存中拷贝该块到 cache 中的行中然后对其经行修改**



实现该方式，需要在 cache 行上加 修改标志位

这种方式虽然可以减少访问主存的次数，但可能会造成数据不一致的隐患



### 全写 (write through)

简单来说就是

- 当命中时，同时更新 cache 和主存中的

- 当未命中时，只更新主存中的内容

注意，这里的更新对于 cache 来说是修改字所在的块，对于 主存 来说就是字

这样做无需加符号位，但是降低了 cache 的功效



### 写一次(write once)

写一次发考虑到了 CPU 的特性，综合了上面两种策略

1. 在第一次写命中时写入主存

   这是因为  CPU 在第一次写 cache 命中时，会在 **总线上启动一个存储写周期**，其他的 cache 在监听到此主存地址以及写信号之后，即可复制该块或者是即使作废，便于维护系统全部 cache 的一致性

2. 后续的方式与 `写回` 相同





## 多级 Cache

奔腾系列 CPU 

![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220411223852.png)

和 cache 与主存之间的关系一样，在访问第一级 cache 缺失后就会访问第二级 cache

一个例子来说明

![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220411224342.png)

1. 时钟频率是 5GHz，时钟周期就是 `0.2 ns`，访问主存消耗的时钟周期就是 `100 / 0.2 = 500` 个

2. 如果加一级缓存

   ```shell
   总 CPI = 基本 CPI + 各条指令在存储器停顿的时钟周期
   	  = 1 + 500 * 2%
   	  = 11
   ```

3. 引入二级缓存之后，二级 cache 的损失缺失为 `5 / 0.2 = 25`

   ```shell
   CPI = 1 + 2% * 25 + 0.5% * 500
       = 4.0
   ```

可以粗略地估计，加上二级 cache 之后，效率可以提高大大约 3 倍



---





## 番外篇 —— Java 中 volatile 的实现原理

> volatile 是轻量级的 sychronized，在多核处理器开发中保证了 共享变量的 可见性，当一个线程修改了一个共享变量之后，另一个线程能够读到这个修改后的值
>
> 它不会引起线程上下文切换和调度，使用得当比 sychronized 更胜一筹

Java语言规范第3版中对 volatile 的定义如下：Java编程语言允许线程访问共享变量，为了 确保共享变量能被准确和一致地更新，线程应该确保通过**排他锁**单独获得这个变量 

如果一个字段被声明成volatile，Java线程内存模型确保所有线程看到这个变量的值是一致的 



在上面我们提到过，CPU 与 cache 之间的操作是透明的，CPU 给出数据的地址对**一个字**的数据经行访问，当 cache 命中时，就会直接返回 cache 中的值，如果未命中，则从主存中读出那个字的数据，并且将主存中那个字所在的数据块刷到 cache 中的某一行

为了规范描述，下面给出与之相关的 CPU 中的术语

![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220407092553.png)

在 《Java 并发编程的艺术》 中，作者通过工具获取到 JIT 编译器为 volatile 变量生成的汇编指令如下

![](https://pic22go.oss-cn-beijing.aliyuncs.com/md/20220407092752.png)

典型的排空栈的指令，只不过是，对于有 volatile 修饰的变量经行写操作时，在 第二句话上会多出 Lock 前缀，通过查表可知，有 Lock 前缀的指令在多核处理器下会触发两个事情

1. 将当前处理器 **cache 行数据** 写回 **系统主存**
2. 上面的操作会使其他CPU(核心)cache中缓存该内存地址的数据无效



上面我们说过，为了提高处理速度，在主存和CPU之间有 cache 的存在，但是，如果修改从 cache 中读出的值，将不会知道何时写入内存中

这个 Lock 前缀的指令，就会将这个变量所在的 **缓存行** 的数据协会到内存中



但是，其他核心缓存地址上的值还是旧的，再执行操作时就会有问题，需要 **缓存一致性** 协议来保证数据的一致性：

每个处理器通过**嗅探在总线上传播的数据**来检查自己缓存的值是不是过期了，当处理器发现自己**缓存行对应的内存地址被修改**，就会将当前处理器的**缓存行设置成无效状态**，当处理器对这个数据进行修改操作的时候，会**重新从系统内存中把数据读到处理器缓存里** (有点 copy-on-write 的味道)



### 两条实现原则

#### Lock 前缀指令会引起处理器缓存写到内存

   Lock 前缀指令在执行指令期间，声言处理器中的 LOCK# 信号以确保在该信号期间，处理器可以 **独占任何共享内存**

   独占共享内存的方法可以通过 **锁总线**(不能访问内存) 或是 **cache 锁定**



   在目前流行的 CPU 中，一般使用 **cache 锁定**的方式，如果访问的内存区域**已经缓存**在 cache 中，则不会声言LOCK#信号；

   相反，它会锁定这块内存区域的缓存并回写到内存，并使用**缓存一致性机制来确保修改的原子性**，缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据 



#### 一个处理器 cache 写入到 主存中会导致其他处理器的缓存失效

`intel` 处理器通过 MESI(修改、独占、共享、无效)控制协议去维护内部缓存和其他处理器缓存的一致性

在多核处理器系统中进行操作的时候，IA-32和Intel 64处理器**能嗅探其他处理器访问系统内存和它们的内部缓存**(可以理解为上面的 写一次 法中在首次写 cache 命中时启动的总线上的写周期，其他的 cache 可以监听这种动作来决定数据的有效性) 

处理器使用嗅探技术保证它的内部缓存、系统内存和其他处理器的缓存的数据在总线上保持一致

例如，如果通过嗅探一个处理器来检测到 **其他处理器** 打算写内存地址，而这个地址处于共享的状态，那么这个处理器中的缓存行将失效，当下次在这个处理器中访问相同的内存地址时，就会执行缓存行填充



### volatile 的使用优化

著名的Java并发编程大师Doug lea在`JDK 7`的并发包里新增一个队列集合类LinkedTransferQueue，它在使用volatile变量时，用一种追加字节的方式来优化队列出队和入队的性能

```java

private transient f?inal PaddedAtomicReference<QNode> head;

private transient f?inal PaddedAtomicReference<QNode> tail;

static final class PaddedAtomicReference <T> extends AtomicReference T> {
	// 15个引用 + 父类的r 追加到64个字节
	Object p0, p1, p2, p3, p4, p5, p6, p7, p8, p9, pa, pb, pc, pd, pe;
	PaddedAtomicReference(T r) {
		super(r);
	}
}
public class AtomicReference <V> implements java.io.Serializable {
	private volatile V value;
	// 省略其他代码
｝

```

对于很多蓝厂的CPU而言， L1/L2甚至是 L3 缓存都是 64个字节占一行，不支持部分填充缓冲行，这意味着，如果队列的 头节点 和 尾节点都不足 64字节 的话，就会被读到 cache 中的同一行中

在多处理器下**每个处理器都会缓存同样的头、尾节点**，当一 个处理器试图修改头节点时，会将整个缓存行锁定 

在 缓存一致性机制 的作用下，会导致其他处理器不能访问 cache 中的尾部节点，只能够通过不断地加打标记然后从 主存中 刷入，大大影响性能

而，入队列和出队列的操作需要对**队头和队尾经行不断地修改**，这样的话 cache 形同虚设



填满缓冲行**避免了头节点和尾节点加载到同一个缓冲行中，使得对其中一个经行修改时意外一个不会受影响被锁定**



下面的情况不推荐使用追加到 64字节 的方法：

1. 缓存行大小不是 64字节宽 的处理器

2. 共享变量不会被频繁地写

   追加更多的字节到 cache 本身就会带来一定的损耗，如果共享变量不会被频繁得写，出现锁的概率也会非常小



这种做法在 JDK7 中可能会失效，因为编译器会淘汰和重拍无用字段，需要使用其他的追加字节的方式



---



## 总结

本文从设计 cache 的原理出发，解释了 cahce 与内存中数据的映射方式，为了提高 cache 的使用效率，进一步介绍了几种基本的替换方法

对数据的操作无非就是读写，cache 存在于 CPU 和主存之间，写操作也会经过，几种写操作策略提供了在 cahce 与主存之间保持一致性的解决方案

随后，我们介绍了多级 cache，这种结构能够进一步释放 cache 的效率



最后，我们窥探了 Java 中的 `volatile` 关键字来看底层对于通过 cache 的写操作的支持，其中使用缓存一致性原理，借助排他锁实现写操作的原子性，以及通过处理器的嗅探技术保证 cache 数据更新实时有效



<br/><br/>

完

<br/><br/><br/><br/>



---

计算机组成原理小计持续更新，欢迎关注 :smile: